\chapter*{2. Methods}
\markboth{Methods}{}
\setcounter{chapter}{2}
\setcounter{section}{0}
\addcontentsline{toc}{chapter}{2. Methods}

The preceding chapter summarised a review of the literature relating to twins, correlations, power and simulations undertaken to inform our approach to analysis.  Through this review we identified a series of suitable approaches and tests for evaluating differences in Pearson and Spearman correlations in two groups. Important considerations were the efficiency of our implemented simulation functions, and a well-designed data structure to support our planned as well as future outputs.  The $R$ programming environment was used for all analyses \cite{R2018}.

\subsection{Fisher's Z test (analytical approach)}
A test statistic for the difference in correlations can be calculated as the difference in Fisher's Z transformed values weighted by the approximate standard error of the difference \cite{Fisher1990,David1938}
$$\hat{\theta} = z_{MZ} - z_{DZ}$$
with standard error,
$$se_{\hat{\theta}}  = \sqrt{(n_{MZ}-3)^{-1}+(n_{DZ}-3)^{-1}} $$
and test statistic,
$$t_{\hat{\theta}}   = \hat{\theta} / se_\hat{\theta}$$ 

The type 2 error rate $\beta$ is estimated through comparison of this test statistic $t_{\hat{\theta}}$ to a reference value on the standard normal distribution.   Using the cumulative normal distribution function $\Phi$ (Phi), the reference score $q$ is calculated as the normal probability quantile corresponding to our $\alpha$ level divided by the 'sidedness' of our test. 
 $$q = \Phi^{-1}(\alpha/\text{sidedness})$$
Where we refer to sidedness, we mean whether we are concerned with a single- or two-tailed probability.  Here, we are testing the hypothesis that $\rho_{MZ} = \rho_{DZ}$ using a two-tailed p-value, implying 'admissible alternatives' to be the case that $\rho_{MZ}$ is greater than or less than $\rho_{DZ}$, that is, $|\rho_{MZ} - \rho_{DZ}| > 0$ \cite{David1938}.  One-tailed consideration, for example that $\rho_{MZ} > \rho_{DZ}$, is not considered in this report however, the functions developed may be parameterised in this way if desired. 
Employing the concepts detailed above, $\beta$ is calculated as 
$$\beta = \Phi \big( q - t_\theta \big)$$
Our power estimate for the detection of difference in correlations is $\power(\theta) = 1 - \beta$. Putting the above altogether, we calculate power using the Fisher's $z$ test statistic as,

$$ \power = 1- \Phi \Bigg(\Phi_{\alpha/2}^{-1} -  \abs\bigg(  \frac{\arctanh(r_{MZ}) - \arctanh(r_{DZ})}{\sqrt{(n_{MZ}-3)^{-1} + (n_{DZ}-3)^{-1}}}  \bigg)  \Bigg) $$ 

The code we used to implement the analytic Fisher's Z test approach to power calculation in $R$ is displayed in listing \ref{lst:fz_nosim}.

\begin{lstlisting}[float=h,caption={Fisher's Z test (analytic approach)},label={lst:fz_nosim}]
# Fishers Z test - no sim
fz_nosim <- function(r1,r2,n1,n2,
                     alpha = 0.05, sidedness=2,method = "pearson",
                     power = TRUE) {
  # Calculate Fisher's Z
  z1     <- atanh(r1)
  z2     <- atanh(r2)
  
  # Take difference
  zdiff  <- z1-z2
  
  # Calculate standard error and test statistic
  z_se   <- sqrt(1/(n1-3) + 1/(n2-3))
  z_test <- zdiff/z_se
  
  # Optionally return p-value for observing diff at least this large under H0
  z_p    <- sidedness*pnorm(-abs(z_test))
  if (power == FALSE) return("p" = z_p)
  z_ref   <- qnorm(1-alpha/sidedness)
  z_power <- 1-pnorm(z_ref - abs(z_test))
  return(z_power)
}
\end{lstlisting}

The above method is the de facto standard, as used for example in the Stata \code{power two correlations} \cite{StataCorp2013}.  However, other options for evaluating the difference in Pearson or Spearman correlations should be considered.  

\subsection{Fisher's Z test (simulation approach)}
Using a simulation approach we take our hypothesis tests and apply them to draws from simulated data designed to mimic our samples through parameterisation using the hypothesised underlying bivariate population distributions.  

So where in our formula we might plug in hypothesised sample coefficients of 0.2 and 0.5, in the simulation we use these values to represent the true correlations in the underlying population from which we draw our samples.  Over a large number of simulations of bivariate twin data the proportion of hypothesis tests returning p-values lower than our type 1 error threshold is our power estimate.

The simulation-based Fisher's Z test function $R$ code is displayed in listing \ref{lst:fz}.

\begin{lstlisting}[float=h,caption={Fisher's Z test (simulation approach)},label={lst:fz}]
# Fishers Z test
fz <- function(a,b,sidedness=2,method = "pearson") {
  # Two samples
  n1 <- nrow(a)
  n2 <- nrow(b)
   
  # Compute z-transformed sample correlation coefficients
  z1     <- atanh(cor(a,method = method)[2,1])
  z2     <- atanh(cor(b,method = method)[2,1])
  zdiff  <- z1-z2
  
  # calculate standard error and test statistic
  z_se   <- sqrt(1/(n1-3) + 1/(n2-3))
  z_test <- zdiff/z_se
  
  # return p-value
  z_p    <- sidedness*pnorm(-abs(z_test))
  return(z_p)
}
\end{lstlisting}


In addition to applying the Fisher Z test in a simulation context, alternate tests we identified and implemented for inclusion in our simulation study were as follows. 

\subsection{Zou's confidence interval}
Zou's confidence interval is used to calculate a confidence interval for the difference in two correlations, and a hypothesis test employing this method is featured in the R package \code{cocor} \cite{Zou2007,Diedenhofen2015}.  A hypothesis test using Zou's confidence interval evaluates whether zero lies within the lower and upper bounds of the interval estimate of the difference in correlations, returning 1 if so or otherwise zero.  Over a run of simulations this would be expected to return identical results to the Fisher Z test, but may be more efficient.

Zou's approach expands on earlier work \cite{Olkin1995} to calculate a confidence interval for a difference in correlations using a so-called Simple Asymptotic approach, using what Zou refers to as a Modified Asymptotic method \cite{Zou2007}.  Both approaches draw heavily on Fisher's earlier work \cite{Fisher1990}.  The modified asymptotic method of Zou consists of first calculating confidence intervals for the two respective z-transformed correlations (transformed as per Fisher's method, described above):
$$(l_z_k, u_z_k) = z_k \pm \sqrt{\frac{1}{n_k - 3}} \times \Phi_{\alpha/2}^{-1},\ \text{where} \ k \in \{1,2\}$$

Then, the lower (L) and upper (U) bounds of the modified asymptotic confidence interval for the difference in correlations are calculated:
$$L = r_1 - r_2 - \sqrt{(r_1 - \tanh(l_z_1))^2 + (\tanh(u_z_2)- r_2)^2}$$
$$U = r_1 - r_2 + \sqrt{(\tanh(u_z_1) - r_1)^2 + (r_2 - \tanh(l_z_2))^2}$$

If zero is within the bounds of the confidence interval for the difference, the test returns as 1, and otherwise 0.

Our implemenation of the Zou's confidence interval test function is displayed in listing \ref{lst:zou}.

\begin{lstlisting}[float=h,caption={Zou's confidence interval},label={lst:zou}]
zou <- function(a,b,alpha = 0.05,sidedness=2,method = "pearson") {
  # From Zou (2007) and used in Cocor (note typo for U in paper; should be '+')
  #  However, really, this is equivalent to fz test for hypothesis testing purposes
  
  # compute z- transformed correlations and differences
  r  <- c(cor(a,method = method)[2,1], cor(b,method = method)[2,1])
  z  <- atanh(r)
  zdiff  <- z[1]-z[2]
  
  # calculate standard error for respective z scores
  n  <- c(nrow(a),nrow(b))
  z_se   <- sqrt(1/(n-3))

  # calculate reference threshold
  z_ref  <- qnorm(1-alpha/sidedness)
  
  # calculate respective confidence intervals
  ci_mat <- matrix(c(-1,-1,1,1),nrow = 2, ncol = 2, dimnames =list(c("Mz","Dz"),c("l","u")))
  z_ci   <- z + ci_mat * z_se * z_ref
  r_ci   <- tanh(z_ci)
  
  # calculate Zou's Modified Asymptoptic confidence interval for difference in correlations
  L      <- r[1]-r[2] - sqrt((r[1]      - r_ci[1,1])^2 + (r_ci[2,2] - r[2]     )^2)
  U      <- r[1]-r[2] + sqrt((r_ci[1,2] - r[1]     )^2 + (r[2]      - r_ci[2,1])^2)
  r_diff_ci <- c(L,U)
  
  # return test value (0 or 1, however, in the power context this resolves to same outcome as p)
  ci_test <- (L < 0) && (0 < U)
  return(c(ci_test,r_diff_ci))
}
\end{lstlisting}



\subsection{Generalised Variable Test}

The generalised variable (GV) test involves transformation of the simulated sample correlations into so-called pivotal quantities the difference of which is used to calculate a test statistic and p-value \cite{Krishnamoorthy2014}. Synthesising two reported approaches \cite{Krishnamoorthy2007,Kazemi2016} this test was first implemented as an example by my supervisor Enes Makalic in a Matlab script, and subsequently adapted by myself as a function in R.  A compiled version using RCPP to leverage C++ routines for random number draws was suggested by my colleague Koen Simons, and adopted to improve the function's run time. However, this later version was not compatible with the parallelised simulation approach, and in this context the non-RCCP 'GVT-r' version was used.

Given two bivariate normal samples $k\in\{1,2\}$, the sample correlation coefficients $r_k$ are used to estimate two respective quantities $r_k^* = \frac{r_k}{\sqrt(1-r_k^2)}$, and the generalised variables $G_{\rho}_k}$:
$$G_{\rho}_k} = \frac{r_k^*\sqrt{W_k} - U_k}{\sqrt{(r_k^*\sqrt(W_k) - U_k)^2 + V_k}}$$
where,
$$U_k \sim N(0,1) ,\ V_k \sim \chi_{n_k - 1}^2 ,\ \text{and} \ W_k \sim \chi_{n_k-2}^2$$
A p-value using the GV test is calculated as twice the value of the smaller of two quantities: the proportion of differences in $G_{\rho}_k}$ less than 0, and the proportion greater than 0.

The GV test function $R$ code is displayed in listing \ref{lst:gvtr}.

\begin{lstlisting}[float=h,caption={GV test (R version)},label={lst:gvtr}]
gvt_r <- function(a,b,M=1e4,method = "pearson") {
  # Two samples
  n1 <- nrow(a)
  n2 <- nrow(b)
  
  # Compute sample correlation coefficients
  r1 <- cor(a,method = method)[2,1]
  r2 <- cor(b,method = method)[2,1]
  r  <- c(r1,r2)
  
  # Generate random numbers
  V2     <- matrix(data=0, nrow = M, ncol = 2)
  V2[,1] <- rchisq(M, df = n1-1, ncp = 1)
  V2[,2] <- rchisq(M, df = n2-1, ncp = 1)
  
  W2     <- matrix(data=0, nrow = M, ncol = 2)
  W2[,1] <- rchisq(M, df = n1-2, ncp = 1)
  W2[,2] <- rchisq(M, df = n2-2, ncp = 1)
  
  Z <-matrix(data = rnorm(2*M), nrow=M, ncol = 2)
  
  # Compute test statistic
  rstar <- r/sqrt(1-r^2)
  top   <- c(sqrt(W2[,1])*rstar[1],sqrt(W2[,2])*rstar[2]) - Z
  G     <- top / sqrt( top^2 + V2 )
  
  # Compute p value
  Grho <- G[,1] - G[,2];
  p    <- 2*min( mean(Grho<0), mean(Grho>0) ); 
  return(p)
}
\end{lstlisting}

\subsection{Signed log-likelihood ratio test}
The signed log likelihood ratio (SLR) test is formulated as the signed difference in sample correlation coefficients multiplied by the square root of the sum of respective coefficients' log-likelihoods.  The test here is a partial implementation of a recently reported modified signed log-likelihood ratio (MSLR) test  for differences in two bivariate normal correlations \cite{Kazemi2016}. The SLR and MSLR tests are well established general hypothesis tests \cite{Barndorff1986,Barndorff1991,Diciccio2001,Krishnamoorthy2014}, the novelty in Kazemi and Jafari's approach being the applied context of difference in correlations. However, we (myself, nor my supervisors) were unable to successfully replicate the 'modified' portion of Kazemi and Jafari's reported algorithm.  Due to time constraints, and noting that the 'unmodified' SLR test appeared to return p-values similar to the other hypothesis tests it was decided that inclusion of the SLR test would be a valid option to consider.

The SLR test function $R$ code is displayed in listing \ref{lst:slr}.

\begin{lstlisting}[float=h,caption={Signed log-likelihood ratio test},label={lst:slr}]
slr <- function(a,b,M=1e4,sidedness=2,method = "pearson") {
  # Signed Log-likelihood Ratio test (an 'unmodified' version of test 
  # described in Krishnamoorthy and Lee, Kazemi and Jafari , DiCiccio etc)
  # Two samples
  n  <- c(nrow(a),nrow(b))
  
  # Compute z-transformed sample correlation coefficients
  r  <- c(cor(a,method = method)[2,1], cor(b,method = method)[2,1])
  z  <- atanh(r)
  
  # Calculate average z as a plug in value
  rf <- tanh(mean(z))
  
  # calcaulte SLR
  slr <-sign(r[1]-r[2])*sqrt(sum(n*log(((1-rf*r)^2)/((1-r^2)*(1-rf^2)))))
  
  # return p-value
  p    <- 2 * (1 - pnorm(abs(slr))); 
  return(p)
}
\end{lstlisting}

\subsection{Permutation test}
The permutation test is a non-parametric approach which compares the absolute 
difference of the Z-transformed sample correlations with those using correlations 
from a series of group membership permutations using the sample rank orders as 
values.  Under a hypothesis of no difference in correlation, those differences arising from permutations would be assumed to be equally likely as those observed, or anticipated to be observed \cite{Efron1993}.  Across a series of $M$ permutations (in this study, 10,000), a $p$-value is calculated as the proportion of permutation derived absolute differences (($\abs(z_{MZ}^* - z_{DZ}^*)$)) of greater magnitude than $\abs(z_{MZ} - z_{DZ})$.

The implementation of this permutation test in $R$ is displayed in listing \ref{lst:pt}.

\begin{lstlisting}[float=h,caption={Permutation test},label={lst:pt}]
pt <- function(a,b,M=1e4,sidedness=2,method = "pearson") {
  # Based on Efron and Tibshirani, 1993
  # Store size, and calculate z-transformed correlations
  n  <- c(nrow(a),nrow(b))
  r  <- c(cor(a,method = method)[2,1], cor(b,method = method)[2,1])
  z  <- atanh(r)
  
  # Store rank-ordered vector representations, in one column
  v  <- cbind(rank(rbind(a[,1],b[,1]),ties.method = "random"),
              rank(rbind(a[,2],b[,2]),ties.method = "random"))
  # label rows
  rownames(v) <- c(rep("A",n[1]),rep("B",n[2]))
  
  # initial empty test vector
  rtest <- numeric(0)
  
  # run M permutations (default is 10,000),
  #  - returns test that absolute magnitude of difference
  #    is at least as great as that of the input z-transformed corr. diff.
  for (i in 1:M){
    permute <- cbind(v,rbinom(sum(n),1,0.5))
    rstar   <- c(cor(permute[permute[,3]==0,c(1,2)],method = method)[2,1],
                 cor(permute[permute[,3]==1,c(1,2)],method = method)[2,1])
    zstar   <- atanh(rstar)
    rtest   <- c(rtest,
                 abs(zstar[1]-zstar[2]) > abs(z[1]-z[2]))
    } 
  
  # return p-value: proportion of test results at least as large as obs'd
  p <- mean(rtest)
  return(p)
}
\end{lstlisting}

\section{Analysis plan}


\section{Simulation}
A function was developed to undertake a single simulation of all of the above tests (listing \ref{lst:corr_diff}).  Each test used the same simulated population as source input for the simulation based tests, while the analytic Fisher Z test result was also returned for reference.

\begin{lstlisting}[float=h,caption={Single run simulation code},label={lst:corr_diff}]
corr_diff_test <- function(rho = c(.2,.5), n = c(30,90), distr = "normal",
                    param1a = c(0,0), param1b = c(0,0),param2a = c(1,1), param2b = c(1,1),
                    alpha = 0.05, sidedness = 2, test = c("fz","gtv","pt","slr","zou"),
                    method ="pearson", lower.tri = FALSE) {
  # cat("Parameters: ",rho[1],rho[2], n[1],n[2], param1a, param1b, param2a, param2b, distr, alpha, sidedness, method,"\n")
  if(lower.tri==TRUE){
    # only calculate lower matrix half when comparing across all correlation combinations
    if(rho[1] < rho[2]) { 
      return(NA)
    }
  }
  results <- list()
  if ("fz_nosim" %in% test) {
    results[["fz_nosim"]] <- fz_ns_compiled(rho[1],rho[2],n[1],n[2], 
                                      alpha = 0.05, sidedness = 2, method = method, power = FALSE)
    if(length(test)==1) return(results)
  }
  require("simstudy")
  a <- genCorGen(n[1], nvars = 2, params1 = param1a, params2 = param2a,  
                dist = distr, corMatrix = matrix(c(1, rho[1], rho[1], 1), ncol = 2), 
                wide = TRUE)[,2:3]
  b <- genCorGen(n[2], nvars = 2, params1 = param1b, params2 = param2b,  
                dist = distr, corMatrix = matrix(c(1, rho[2], rho[2], 1), ncol = 2), 
                wide = TRUE)[,2:3]
  if ("fz"       %in% test) results[["fz"]]       <- fz_compiled(a,b)
  if ("gtv"      %in% test) results[["gtv"]]      <- gtv(a,b) # uses rccp ; so elsewise compiled
  if ("gtvr"     %in% test) results[["gtvr"]]      <- gtv_compiled(a,b) 
  if ("pt"       %in% test) results[["pt"]]       <- pt_compiled(a,b)
  if ("slr"      %in% test) results[["slr"]]      <- slr_compiled(a,b)
  if ("zou"      %in% test) results[["zou"]]      <- zou_compiled(a,b)[1]
  return(rbind(results[test]))
}
\end{lstlisting}

  
\section{Feasability appraisal}
To undertake the simulations as planned for combinations of 
\begin{itemize}
  \item correlations at .05 intervals 
  \item exponentially increasing group sizes of 15 through 960
  \item three bivariate distribution types being normal, and gamma with mild skew and extreme skew
  \item two approaches to correlation measurement 
  \item across 6 tests
  \item with 1000 simulations per combination 
\end{itemize}

Briefly summarise timing tests
\\
Point to appendix for timing test results

would have resulted in more than 2 billion results and taken 33 years.  Instead, I reduced my ambition using a .1 correlation resolution and dropped the permutation test which was implemented inefficiently to get this to 16 days.

\section{Revision}
  
results in format of images
\\
\\
contour plot
\\
\\  
interpolation using monotonic increasing spline function
\\
\\
Required sample size to achieve 80% power
\\
\\  
Difference to achieve 80% power

